{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrTvzSyuepmO"
   },
   "source": [
    "# Sentiment Analysis Using Multinomial Logistic Regression\n",
    "\n",
    "In this project, you’ll use the Twitter Tweets Sentiment Dataset for sentiment analysis with multinomial logistic regression to classify tweets as \n",
    "positive, negative, or neutral.\n",
    "\n",
    "The Twitter Tweets Sentiment Dataset comprises 27,481 tweets with 8,582 positive tweets, 7,781 negative tweets, and 11,118 neutral tweets. The corpus \n",
    "comprises the following columns:\n",
    "\n",
    "textID: This is the unique ID of the tweet.\n",
    "\n",
    "text: This is the complete text of the tweet.\n",
    "\n",
    "selected_text: This is a word or phrase selected from the tweet that expresses the sentiment.\n",
    "\n",
    "sentiment: This is the sentiment of the tweet (positive, negative, or neutral).\n",
    "\n",
    "Note: In this project, we’ll only use 6000 tweets, 2000 each from positive, negative, and neutral classes. This is done to keep the model training and\n",
    "testing times manageable. Furthermore, we have dropped the “textID” and “selected_text” columns from the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q03g2NF58ZV1"
   },
   "source": [
    "## Task 1: Import Libraries\n",
    "\n",
    "Let’s start the project by importing the required libraries. In this project, you’ll make use of several libraries, including string, numpy, pandas,\n",
    "tqdm, and sklearn. We will also use functions to calculate metrics such as confusion matrix and classification report, and to plot results using\n",
    "matplotlib.\n",
    "\n",
    "Import the following libraries to complete this task:\n",
    "\n",
    "1. string: To perform common string operations\n",
    "\n",
    "2. numpy: For numerical computing\n",
    "\n",
    "3. pandas: For data analysis and manipulation\n",
    "\n",
    "4. tqdm: To show a progress bar during model training\n",
    "\n",
    "5. matplotlib: To plot results and visualize metrics and other performance measures\n",
    "\n",
    "6. sklearn.model_selection: To split datasets into train and test sets\n",
    "\n",
    "7. sklearn.feature_extraction.text: To extract features from text data, such as the CountVectorizer, which is used for converting text into a matrix \n",
    "of token counts\n",
    "\n",
    "8. sklearn.metrics: To evaluate the model’s performance, including functions to compute confusion matrix and classification report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3048,
     "status": "ok",
     "timestamp": 1684441475039,
     "user": {
      "displayName": "Mudasir Yasin",
      "userId": "02181408715486485594"
     },
     "user_tz": -300
    },
    "id": "yI1xmb47mo-C"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biztdTex5VuQ"
   },
   "source": [
    "## Task 2: Load the Dataset\n",
    "\n",
    "In this task, load the dataset file into a pandas DataFrame. The dataset comprises two columns “sentiment,” and “tweet,” The “tweet” column contains\n",
    "the tweet text and the “sentiment” column contains its label (“positive”, “negative”, “neutral”).\n",
    "\n",
    "Follow the given steps to complete this task:\n",
    "\n",
    "1. Read the dataset file named Tweets.csv into the pandas DataFrame.\n",
    "    \n",
    "2. Print the first few rows of the DataFrame to verify the data inside the DataFrame.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment                                              tweet\n",
      "0   neutral  Happy Monday tweeples... hope it wasn`t to har...\n",
      "1  negative  I can`t believe I thought I had a morning shif...\n",
      "2   neutral  So its Superstar Sunday?  is one superstar I k...\n",
      "3  positive   yay i hit 50 subscribers on youtube. go me  lol.\n",
      "4   neutral                               Today = marking *135\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into a pandas DataFrame\n",
    "df = pd.read_csv('/usercode/Tweets.csv')\n",
    "\n",
    "# Print the DataFrame head\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY8pthmS5pjN"
   },
   "source": [
    "## Task 3: Remove Punctuation from Tweets\n",
    "\n",
    "In this task, remove the punctuation characters from the tweets. This will help simplify the data since punctuation marks do not usually carry\n",
    "significant meaning in the context of text analysis. This will also reduce noise in the data and the dimensionality of the feature space, which can\n",
    "lead to a more efficient and effective model.\n",
    "\n",
    "To complete this task, perform the following operations:\n",
    "\n",
    "1. Create a function that takes a string text as input and removes any punctuation characters from it.\n",
    "\n",
    "2. Apply the function to the first column, “tweet,” of the DataFrame.\n",
    "    \n",
    "3. Print the first few rows of the modified DataFrame to verify the change.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1684441500641,
     "user": {
      "displayName": "Mudasir Yasin",
      "userId": "02181408715486485594"
     },
     "user_tz": -300
    },
    "id": "C_MQnLQ15sHx",
    "outputId": "b81cfccc-f021-4fda-abe0-91712c76add7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment                                              tweet\n",
      "0   neutral  Happy Monday tweeples hope it wasnt to hard to...\n",
      "1  negative  I cant believe I thought I had a morning shift...\n",
      "2   neutral  So its Superstar Sunday  is one superstar I kn...\n",
      "3  positive     yay i hit 50 subscribers on youtube go me  lol\n",
      "4   neutral                                 Today  marking 135\n"
     ]
    }
   ],
   "source": [
    "# Define a function to remove punctuation from a string\n",
    "def remove_punctuation(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "# Apply the function to the first column of the DataFrame\n",
    "df.iloc[:, 1] = df.iloc[:, 1].apply(remove_punctuation)\n",
    "\n",
    "# Print the DataFrame head\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq0ccz8N5sTl"
   },
   "source": [
    "## Task 4: Split Tweets into a Bag of Words\n",
    "\n",
    "After removing the punctuation from each tweet, you’ll split each string value in the “tweet” column into a bag of words and update the column with \n",
    "the modified values.\n",
    "\n",
    "Perform the following operations to complete this task:\n",
    "\n",
    "1. Apply a lambda function to the “tweet” column of the DataFrame. The lambda function should split each string in the tweet column into a bag of words.\n",
    "\n",
    "2. Print the first few rows of the modified DataFrame to verify the change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1684441500641,
     "user": {
      "displayName": "Mudasir Yasin",
      "userId": "02181408715486485594"
     },
     "user_tz": -300
    },
    "id": "MI_GuC4l54Gn",
    "outputId": "a16d8bcc-e563-4c62-e6e9-8829333dfac0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment                                              tweet\n",
      "0   neutral  [Happy, Monday, tweeples, hope, it, wasnt, to,...\n",
      "1  negative  [I, cant, believe, I, thought, I, had, a, morn...\n",
      "2   neutral  [So, its, Superstar, Sunday, is, one, supersta...\n",
      "3  positive  [yay, i, hit, 50, subscribers, on, youtube, go...\n",
      "4   neutral                              [Today, marking, 135]\n"
     ]
    }
   ],
   "source": [
    "# Split each tweet into a bag of words\n",
    "df['tweet'] = df['tweet'].apply(lambda x: x.split())\n",
    "\n",
    "# Print the DataFrame head\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRtUFzHz53na"
   },
   "source": [
    "## Task 5: Create a Vocabulary and Remove Stop Words\n",
    "\n",
    "In this task, we create a vocabulary of all words that have occurred in the tweets. Then remove the stop words from this vocabulary.\n",
    "\n",
    "Stop words are common words in a language (e.g., “the,” “is,” “and,” etc.) that do not carry significant meaning and are often used in various \n",
    "contexts. Removing stop words helps to reduce noise in the text data, as these words are unlikely to contribute much to the overall understanding or\n",
    "sentiment of the tweet. Furthermore, by removing them, you can reduce the size of the feature space and the computational resources required for \n",
    "training and inference. This can lead to faster model training and prediction.\n",
    "\n",
    "Follow the given steps to remove the stop words:\n",
    "\n",
    "1. Count the frequencies of all words that have occurred in the tweets in a dictionary.\n",
    "\n",
    "2. Sort the dictionary by its values in descending order.\n",
    "\n",
    "3. Remove the 100 most frequent words.\n",
    "\n",
    "4. Store the remaining words and their frequencies in a new dictionary.\n",
    "\n",
    "5. Extract the keys from the dictionary and assign them to a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1684441500642,
     "user": {
      "displayName": "Mudasir Yasin",
      "userId": "02181408715486485594"
     },
     "user_tz": -300
    },
    "id": "HIB6iC6xsOWS"
   },
   "outputs": [],
   "source": [
    "# Count occurrences of each word\n",
    "vocabulary_dict = {}\n",
    "for row in df['tweet']:\n",
    "    for word in row:    \n",
    "        if word.lower() in vocabulary_dict:\n",
    "            vocabulary_dict[word.lower()] += 1\n",
    "        else:\n",
    "            vocabulary_dict[word.lower()] = 1\n",
    "\n",
    "# Remove the stop words\n",
    "vocabulary_dict = sorted(vocabulary_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "vocabulary_dict = vocabulary_dict[100:]\n",
    "vocabulary_dict = dict(vocabulary_dict)\n",
    "\n",
    "# Extract keys from the dictionary\n",
    "vocabulary = list(vocabulary_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10563"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sorry', '1', 'no1', 'brinn', 'passtime')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[0] , vocabulary[100] , vocabulary[1000] , vocabulary[10000] , vocabulary[10562]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZkcupGokrU4"
   },
   "source": [
    "## Task 6: Create Feature Vectors\n",
    "\n",
    "Now that you’ve created a vocabulary and removed the stop words, use it to create feature vectors for the tweets. Then convert the resulting \n",
    "sparse matrix to a dense NumPy array. This array will later be used for training and testing the model.\n",
    "\n",
    "Follow the given steps to create feature vectors for tweets:\n",
    "\n",
    "1. For each tweet in the “tweet” column of the DataFrame, convert each word to lowercase and join them back into a string.\n",
    "\n",
    "2. Tokenize the tweets and create feature vectors based on the vocabulary. The resulting tweet vector will be a sparse matrix representation of the \n",
    "tweets.\n",
    "\n",
    "3. Convert the tweet vectors sparse matrix to a dense numpy array. The resulting array will contain the feature vectors for each tweet.\n",
    "                                                                                                                       \n",
    "4. Print the NumPy array.\n",
    "                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 646,
     "status": "ok",
     "timestamp": 1684441501284,
     "user": {
      "displayName": "Mudasir Yasin",
      "userId": "02181408715486485594"
     },
     "user_tz": -300
    },
    "id": "8SqhYlIeRnpX",
    "outputId": "6e25000b-7e97-452e-d133-ada6fd750ef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Extract tweets and convert to lowercase\n",
    "tweets = [' '.join([word.lower() for word in tweet]) for tweet in df['tweet']]\n",
    "\n",
    "# Create the CountVectorizer with the vocabulary\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "# Fit and transform the tweets into feature vectors\n",
    "tweet_vectors = vectorizer.fit_transform(tweets)\n",
    "\n",
    "# Convert tweet_vectors to NumPy array\n",
    "X = tweet_vectors.toarray()\n",
    "\n",
    "# Print the NumPy array\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnjklE3-6C7N"
   },
   "source": [
    "## Task 7: Map and Extract the Sentiment Column\n",
    "\n",
    "In this task, map the sentiment labels in the “sentiment” column of the DataFrame to numerical values. Then extract the mapped values into a \n",
    "NumPy array.\n",
    "\n",
    "This mapping of string labels to numerical values ensures compatibility with the model and allows for effective processing. It also enables the use of\n",
    "categorical encoding techniques, such as one-hot encoding, that you’ll use later in this project.\n",
    "\n",
    "Follow the given steps to complete this task:\n",
    "\n",
    "1. Replace the original sentiment labels in the “sentiment” column with their corresponding integer values \n",
    "   (2 for positive, 1 for neutral, and 0 for negative).\n",
    "\n",
    "2. Extract the values from the modified “sentiment” column into a NumPy array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1684441501284,
     "user": {
      "displayName": "Mudasir Yasin",
      "userId": "02181408715486485594"
     },
     "user_tz": -300
    },
    "id": "k5QZH2yMnTsp"
   },
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "\n",
    "# Map the values in the column using the mapping dictionary\n",
    "df['sentiment'] = df['sentiment'].map(mapping)\n",
    "\n",
    "# Extract sentiment from the DataFrame\n",
    "y = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exjc7yQ05cyI"
   },
   "source": [
    "## Task 8: Split the Dataset into Training and Test Sets\n",
    "\n",
    "In this task, split the feature vectors and the corresponding sentiment labels into training and testing subsets in an 80:20 ratio, with \n",
    "stratified sampling to preserve the distribution of sentiment labels. The training dataset will be used to train the model, and the testing dataset\n",
    "will be used to evaluate the model’s performance after training is complete as an independent evaluation set.\n",
    "\n",
    "Stratified sampling ensures that the splitting process maintains the same proportion of sentiment labels in training and testing subsets. \n",
    "This is useful when dealing with imbalanced datasets to ensure a representative distribution of the classes in the subsets.\n",
    "\n",
    "To complete this task, perform the following operations:\n",
    "\n",
    "1. Split the feature vectors and the array of corresponding sentiment labels into training and testing subsets in a ratio of 80:20.\n",
    "\n",
    "2. Print the shapes of the resulting subsets to verify the split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1684441501285,
     "user": {
      "displayName": "Mudasir Yasin",
      "userId": "02181408715486485594"
     },
     "user_tz": -300
    },
    "id": "hd6bfOQ75gM5",
    "outputId": "a3e0073c-8e4e-4fbd-94ed-913751f418ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 10563)\n",
      "(1200, 10563)\n",
      "(4800,)\n",
      "(1200,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing subsets with stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Print the shapes of resulting subsets\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pneAG9N5xuuX"
   },
   "source": [
    "## Task 9: Define the Weights Initialization Function\n",
    "\n",
    "In this task, initialize the weights for the model using the number of features and classes.\n",
    "\n",
    "To complete this task, create a function initialize_weights() that takes two parameters: n_features and n_classes. Inside the function,\n",
    "do the following:\n",
    "\n",
    "1. Create a NumPy array of zeros with a shape of (n_features, n_classes), where n_features represents the number of features and n_classes represents\n",
    "the number of classes.\n",
    "    \n",
    "2. Return the created array, which serves as the initialized weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1684441501285,
     "user": {
      "displayName": "Mudasir Yasin",
      "userId": "02181408715486485594"
     },
     "user_tz": -300
    },
    "id": "iGPr6BtO3gBi"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(n_features, n_classes):\n",
    "    return np.zeros((n_features, n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Define One-Hot Encoding Function\n",
    "\n",
    "In this task, write the one_hot_encode() function to convert an array of class labels y into a one-hot encoded representation. The resulting array\n",
    "will have dimensions (n_samples, n_classes), where each row will represent a sample and each column will represent a class. The elements corresponding\n",
    "to the class labels in y will be marked as 1, while the rest will be 0.\n",
    "\n",
    "To complete this task, create a function one_hot_encode() that takes two parameters: y and n_classes. Here, y is the array of class labels and\n",
    "n_classes represents the number of classes. Inside the function, do the following:\n",
    "\n",
    "1. Create a NumPy array y_encoded, with a shape of (n_samples, n_classes), filled with zeros, where n_samples represents the number of samples.\n",
    "\n",
    "2. Perform one-hot encoding for each sample by marking the corresponding class position with 1 and leaving the rest as 0.\n",
    "\n",
    "3. Return the y_encoded array, representing the one-hot encoded version of the input array y.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, n_classes):\n",
    "    n_samples = len(y)\n",
    "    y_encoded = np.zeros((n_samples, n_classes))\n",
    "    for i in range(n_samples):\n",
    "        y_encoded[i, y[i]] = 1\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11: Define the Softmax Function\n",
    "\n",
    "In this task, write the softmax() function that takes an input array x, computes the softmax activation for each element, and returns the resulting\n",
    "array of normalized values. The softmax function is commonly used in multiclass classification problems to convert raw scores or logits into\n",
    "probabilities representing class probabilities.\n",
    "\n",
    "To complete this task, create a function softmax() that applies the softmax activation function to an input array x and returns the resulting array \n",
    "of normalized values, representing the output of the softmax function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 12: Define the Gradient Descent Function\n",
    "\n",
    "In this task, implement the gradient_descent() function to apply gradient descent to update the weights and the bias of our model based on the \n",
    "feature matrix X, the one-hot encoded target values y_encoded, the initial weights, the initial bias, and the learning rate.\n",
    "\n",
    "To complete this task, create a function gradient_descent() that takes five parameters: X, y_encoded, weights, bias, and learning_rate. Inside the\n",
    "function, do the following:\n",
    "\n",
    "1. Compute the scores by performing matrix multiplication between the feature matrix X and the weights matrix, and add the bias term. Store the result in \n",
    "the scores array.\n",
    "    \n",
    "2. Calculate the probabilities by applying the softmax function to the scores array.\n",
    "\n",
    "3. Compute the error by subtracting the one-hot encoded target values y_encoded from the predicted probabilities.\n",
    "\n",
    "4. Compute the gradient of the weights by multiplying the transpose of the feature matrix X with the error and dividing it by the number of samples.\n",
    "\n",
    "5. Compute the gradient of the bias by summing the error along the first axis and dividing it by the number of samples.\n",
    "\n",
    "6. Update the weights by subtracting the learning rate multiplied by the gradient of the weights.\n",
    "    \n",
    "7. Update the bias by subtracting the learning rate multiplied by the bias gradient.\n",
    "\n",
    "8. Return the updated weights and bias as the output of the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y_encoded, weights, bias, learning_rate):\n",
    "    n_samples = len(X)\n",
    "    scores = np.dot(X, weights) + bias\n",
    "    probabilities = softmax(scores)\n",
    "    error = probabilities - y_encoded\n",
    "    dw = np.dot(X.T, error) / n_samples\n",
    "    db = np.sum(error, axis=0) / n_samples\n",
    "    weights -= learning_rate * dw\n",
    "    bias -= learning_rate * db\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 13: Define the Training Function\n",
    "\n",
    "In this task, write the train_multinomial_logistic_regression() function to train the multinomial logistic regression model by performing gradient \n",
    "descent updates on the weights and bias.\n",
    "\n",
    "To complete this task, create a function, train_multinomial_logistic_regression(), that takes four parameters: X (the input features),\n",
    "y (the target variable), learning rate, and maximum iterations. Inside the function, do the following:\n",
    "\n",
    "1. Initialize the weights matrix by calling the initialize_weights function, passing n_features and n_classes as parameters.\n",
    "\n",
    "2. Initialize the bias vector as an array of zeros with a length equal to the number of classes.\n",
    "\n",
    "3. Encode the target array y into a one-hot encoded representation using the one_hot_encode function, passing y and n_classes as parameters. \n",
    "Store the resulting encoded array as y_encoded.\n",
    "\n",
    "4. Update the weights and bias by calling the gradient_descent function max_iterations times. Pass the feature matrix X, the one-hot encoded \n",
    "target values y_encoded, the weights, the bias, and the learning rate as parameters.\n",
    "\n",
    "5. Return the updated weights and bias as the output of the function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multinomial_logistic_regression(X, y, learning_rate, max_iterations):\n",
    "    n_samples, n_features = np.shape(X)\n",
    "    n_classes = len(np.unique(y))\n",
    "    \n",
    "    weights = initialize_weights(n_features, n_classes)\n",
    "    bias = np.zeros(n_classes)\n",
    "    y_encoded = one_hot_encode(y, n_classes)\n",
    "    \n",
    "    for _ in tqdm(range(max_iterations), desc=\"Training\", unit=\"iteration\"):\n",
    "        weights, bias = gradient_descent(X, y_encoded, weights, bias, learning_rate)\n",
    "    \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 14: Define the Prediction Function\n",
    "\n",
    "In this task, write the predict() function that takes a feature matrix X, the weights, and the bias of our multinomial logistic regression model.\n",
    "It’ll compute the scores and probabilities for each sample and return the predicted class labels based on the maximum probability for each sample.\n",
    "\n",
    "To complete this task, create a function, predict(), that takes three parameters: X, weights, and bias to make predictions using the trained\n",
    "multinomial logistic regression model. Inside the function, do the following:\n",
    "\n",
    "1. Compute the scores by performing matrix multiplication between the feature matrix X and the weights matrix, and add the bias term. Store the result \n",
    "in the scores array.\n",
    "    \n",
    "2. Calculate the probabilities by applying the softmax() function to the scores array.\n",
    "    \n",
    "3. Find the index of the maximum probability for each sample along the second axis. This corresponds to the predicted class label for each sample.\n",
    "                                                                                                                                 \n",
    "4. Return the array of predicted class labels.\n",
    "                                                                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights, bias):\n",
    "    scores = np.dot(X, weights) + bias\n",
    "    probabilities = softmax(scores)\n",
    "    return np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAnD-i5u0RXb"
   },
   "source": [
    "## Task 15: Train the Model\n",
    "\n",
    "Now that you’ve written all the functions to implement multinomial logistic regression, it is time to train the model on the training data using a \n",
    "specified learning rate and a maximum number of iterations. The resulting trained weights and bias will be stored and used to make predictions \n",
    "using the test data.\n",
    "\n",
    "To complete this task, invoke the train_multinomial_logistic_regression() function to train the multinomial logistic regression model using the\n",
    "training data X_train and y_train. Also, pass the learning rate and a maximum number of iterations as parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BdYeyN6W3Rvd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/500 [00:00<?, ?iteration/s]\r",
      "Training:   0%|          | 1/500 [00:00<06:08,  1.35iteration/s]\r",
      "Training:   0%|          | 2/500 [00:01<05:58,  1.39iteration/s]\r",
      "Training:   1%|          | 3/500 [00:02<05:58,  1.39iteration/s]\r",
      "Training:   1%|          | 4/500 [00:02<05:54,  1.40iteration/s]\r",
      "Training:   1%|          | 5/500 [00:03<05:52,  1.40iteration/s]\r",
      "Training:   1%|          | 6/500 [00:04<05:51,  1.41iteration/s]\r",
      "Training:   1%|▏         | 7/500 [00:05<05:50,  1.41iteration/s]\r",
      "Training:   2%|▏         | 8/500 [00:05<05:48,  1.41iteration/s]\r",
      "Training:   2%|▏         | 9/500 [00:06<05:46,  1.42iteration/s]\r",
      "Training:   2%|▏         | 10/500 [00:07<05:47,  1.41iteration/s]\r",
      "Training:   2%|▏         | 11/500 [00:07<05:46,  1.41iteration/s]\r",
      "Training:   2%|▏         | 12/500 [00:08<05:46,  1.41iteration/s]\r",
      "Training:   3%|▎         | 13/500 [00:09<05:44,  1.41iteration/s]\r",
      "Training:   3%|▎         | 14/500 [00:09<05:44,  1.41iteration/s]\r",
      "Training:   3%|▎         | 15/500 [00:10<05:43,  1.41iteration/s]\r",
      "Training:   3%|▎         | 16/500 [00:11<05:44,  1.40iteration/s]\r",
      "Training:   3%|▎         | 17/500 [00:12<05:41,  1.41iteration/s]\r",
      "Training:   4%|▎         | 18/500 [00:12<05:40,  1.41iteration/s]\r",
      "Training:   4%|▍         | 19/500 [00:13<05:39,  1.42iteration/s]\r",
      "Training:   4%|▍         | 20/500 [00:14<05:38,  1.42iteration/s]\r",
      "Training:   4%|▍         | 21/500 [00:14<05:36,  1.42iteration/s]\r",
      "Training:   4%|▍         | 22/500 [00:15<05:45,  1.38iteration/s]\r",
      "Training:   5%|▍         | 23/500 [00:16<06:33,  1.21iteration/s]\r",
      "Training:   5%|▍         | 24/500 [00:17<06:23,  1.24iteration/s]\r",
      "Training:   5%|▌         | 25/500 [00:18<06:07,  1.29iteration/s]\r",
      "Training:   5%|▌         | 26/500 [00:18<05:56,  1.33iteration/s]\r",
      "Training:   5%|▌         | 27/500 [00:19<05:48,  1.36iteration/s]\r",
      "Training:   6%|▌         | 28/500 [00:20<05:43,  1.37iteration/s]\r",
      "Training:   6%|▌         | 29/500 [00:20<05:39,  1.39iteration/s]\r",
      "Training:   6%|▌         | 30/500 [00:21<05:36,  1.40iteration/s]\r",
      "Training:   6%|▌         | 31/500 [00:22<05:34,  1.40iteration/s]\r",
      "Training:   6%|▋         | 32/500 [00:23<05:33,  1.41iteration/s]\r",
      "Training:   7%|▋         | 33/500 [00:23<05:31,  1.41iteration/s]\r",
      "Training:   7%|▋         | 34/500 [00:24<05:29,  1.42iteration/s]\r",
      "Training:   7%|▋         | 35/500 [00:25<05:28,  1.42iteration/s]\r",
      "Training:   7%|▋         | 36/500 [00:25<05:27,  1.42iteration/s]\r",
      "Training:   7%|▋         | 37/500 [00:26<05:26,  1.42iteration/s]\r",
      "Training:   8%|▊         | 38/500 [00:27<05:26,  1.41iteration/s]\r",
      "Training:   8%|▊         | 39/500 [00:28<05:25,  1.42iteration/s]\r",
      "Training:   8%|▊         | 40/500 [00:28<05:23,  1.42iteration/s]\r",
      "Training:   8%|▊         | 41/500 [00:29<05:25,  1.41iteration/s]\r",
      "Training:   8%|▊         | 42/500 [00:30<05:23,  1.42iteration/s]\r",
      "Training:   9%|▊         | 43/500 [00:30<05:21,  1.42iteration/s]\r",
      "Training:   9%|▉         | 44/500 [00:31<05:20,  1.42iteration/s]\r",
      "Training:   9%|▉         | 45/500 [00:32<05:20,  1.42iteration/s]\r",
      "Training:   9%|▉         | 46/500 [00:32<05:19,  1.42iteration/s]\r",
      "Training:   9%|▉         | 47/500 [00:33<05:17,  1.42iteration/s]\r",
      "Training:  10%|▉         | 48/500 [00:34<05:16,  1.43iteration/s]\r",
      "Training:  10%|▉         | 49/500 [00:35<05:14,  1.43iteration/s]\r",
      "Training:  10%|█         | 50/500 [00:35<05:14,  1.43iteration/s]\r",
      "Training:  10%|█         | 51/500 [00:36<05:14,  1.43iteration/s]\r",
      "Training:  10%|█         | 52/500 [00:37<05:15,  1.42iteration/s]\r",
      "Training:  11%|█         | 53/500 [00:37<05:15,  1.42iteration/s]\r",
      "Training:  11%|█         | 54/500 [00:38<05:13,  1.42iteration/s]\r",
      "Training:  11%|█         | 55/500 [00:39<05:11,  1.43iteration/s]\r",
      "Training:  11%|█         | 56/500 [00:39<05:11,  1.43iteration/s]\r",
      "Training:  11%|█▏        | 57/500 [00:40<05:10,  1.42iteration/s]\r",
      "Training:  12%|█▏        | 58/500 [00:41<05:10,  1.43iteration/s]\r",
      "Training:  12%|█▏        | 59/500 [00:42<05:09,  1.42iteration/s]\r",
      "Training:  12%|█▏        | 60/500 [00:42<05:09,  1.42iteration/s]\r",
      "Training:  12%|█▏        | 61/500 [00:43<05:08,  1.42iteration/s]\r",
      "Training:  12%|█▏        | 62/500 [00:44<05:07,  1.43iteration/s]\r",
      "Training:  13%|█▎        | 63/500 [00:44<05:06,  1.43iteration/s]\r",
      "Training:  13%|█▎        | 64/500 [00:45<05:07,  1.42iteration/s]\r",
      "Training:  13%|█▎        | 65/500 [00:46<05:06,  1.42iteration/s]\r",
      "Training:  13%|█▎        | 66/500 [00:47<05:05,  1.42iteration/s]\r",
      "Training:  13%|█▎        | 67/500 [00:47<05:05,  1.42iteration/s]\r",
      "Training:  14%|█▎        | 68/500 [00:48<05:04,  1.42iteration/s]\r",
      "Training:  14%|█▍        | 69/500 [00:49<05:03,  1.42iteration/s]\r",
      "Training:  14%|█▍        | 70/500 [00:49<05:01,  1.42iteration/s]\r",
      "Training:  14%|█▍        | 71/500 [00:50<05:01,  1.42iteration/s]\r",
      "Training:  14%|█▍        | 72/500 [00:51<05:01,  1.42iteration/s]\r",
      "Training:  15%|█▍        | 73/500 [00:51<05:00,  1.42iteration/s]\r",
      "Training:  15%|█▍        | 74/500 [00:52<04:59,  1.42iteration/s]\r",
      "Training:  15%|█▌        | 75/500 [00:53<04:58,  1.42iteration/s]\r",
      "Training:  15%|█▌        | 76/500 [00:54<04:58,  1.42iteration/s]\r",
      "Training:  15%|█▌        | 77/500 [00:54<04:55,  1.43iteration/s]\r",
      "Training:  16%|█▌        | 78/500 [00:55<04:55,  1.43iteration/s]\r",
      "Training:  16%|█▌        | 79/500 [00:56<04:54,  1.43iteration/s]\r",
      "Training:  16%|█▌        | 80/500 [00:56<04:54,  1.43iteration/s]\r",
      "Training:  16%|█▌        | 81/500 [00:57<04:55,  1.42iteration/s]\r",
      "Training:  16%|█▋        | 82/500 [00:58<04:54,  1.42iteration/s]\r",
      "Training:  17%|█▋        | 83/500 [00:58<04:53,  1.42iteration/s]\r",
      "Training:  17%|█▋        | 84/500 [00:59<04:54,  1.41iteration/s]\r",
      "Training:  17%|█▋        | 85/500 [01:00<04:53,  1.41iteration/s]\r",
      "Training:  17%|█▋        | 86/500 [01:01<04:52,  1.41iteration/s]\r",
      "Training:  17%|█▋        | 87/500 [01:01<04:51,  1.42iteration/s]\r",
      "Training:  18%|█▊        | 88/500 [01:02<04:50,  1.42iteration/s]\r",
      "Training:  18%|█▊        | 89/500 [01:03<04:49,  1.42iteration/s]\r",
      "Training:  18%|█▊        | 90/500 [01:03<04:48,  1.42iteration/s]\r",
      "Training:  18%|█▊        | 91/500 [01:04<04:47,  1.42iteration/s]\r",
      "Training:  18%|█▊        | 92/500 [01:05<04:45,  1.43iteration/s]\r",
      "Training:  19%|█▊        | 93/500 [01:06<04:44,  1.43iteration/s]\r",
      "Training:  19%|█▉        | 94/500 [01:06<04:44,  1.43iteration/s]\r",
      "Training:  19%|█▉        | 95/500 [01:07<04:44,  1.42iteration/s]\r",
      "Training:  19%|█▉        | 96/500 [01:08<04:43,  1.42iteration/s]\r",
      "Training:  19%|█▉        | 97/500 [01:08<04:43,  1.42iteration/s]\r",
      "Training:  20%|█▉        | 98/500 [01:09<04:41,  1.43iteration/s]\r",
      "Training:  20%|█▉        | 99/500 [01:10<04:40,  1.43iteration/s]\r",
      "Training:  20%|██        | 100/500 [01:10<04:39,  1.43iteration/s]\r",
      "Training:  20%|██        | 101/500 [01:11<04:38,  1.43iteration/s]\r",
      "Training:  20%|██        | 102/500 [01:12<04:38,  1.43iteration/s]\r",
      "Training:  21%|██        | 103/500 [01:13<04:37,  1.43iteration/s]\r",
      "Training:  21%|██        | 104/500 [01:13<04:37,  1.43iteration/s]\r",
      "Training:  21%|██        | 105/500 [01:14<04:39,  1.41iteration/s]\r",
      "Training:  21%|██        | 106/500 [01:15<04:37,  1.42iteration/s]\r",
      "Training:  21%|██▏       | 107/500 [01:15<04:37,  1.42iteration/s]\r",
      "Training:  22%|██▏       | 108/500 [01:16<04:36,  1.42iteration/s]\r",
      "Training:  22%|██▏       | 109/500 [01:17<04:37,  1.41iteration/s]\r",
      "Training:  22%|██▏       | 110/500 [01:17<04:36,  1.41iteration/s]\r",
      "Training:  22%|██▏       | 111/500 [01:18<04:34,  1.42iteration/s]\r",
      "Training:  22%|██▏       | 112/500 [01:19<04:34,  1.41iteration/s]\r",
      "Training:  23%|██▎       | 113/500 [01:20<04:32,  1.42iteration/s]\r",
      "Training:  23%|██▎       | 114/500 [01:20<04:31,  1.42iteration/s]\r",
      "Training:  23%|██▎       | 115/500 [01:21<04:31,  1.42iteration/s]\r",
      "Training:  23%|██▎       | 116/500 [01:22<04:31,  1.41iteration/s]\r",
      "Training:  23%|██▎       | 117/500 [01:22<04:30,  1.42iteration/s]\r",
      "Training:  24%|██▎       | 118/500 [01:23<04:29,  1.42iteration/s]\r",
      "Training:  24%|██▍       | 119/500 [01:24<04:28,  1.42iteration/s]\r",
      "Training:  24%|██▍       | 120/500 [01:25<04:27,  1.42iteration/s]\r",
      "Training:  24%|██▍       | 121/500 [01:25<04:25,  1.43iteration/s]\r",
      "Training:  24%|██▍       | 122/500 [01:26<04:25,  1.42iteration/s]\r",
      "Training:  25%|██▍       | 123/500 [01:27<04:25,  1.42iteration/s]\r",
      "Training:  25%|██▍       | 124/500 [01:27<04:24,  1.42iteration/s]\r",
      "Training:  25%|██▌       | 125/500 [01:28<04:24,  1.42iteration/s]\r",
      "Training:  25%|██▌       | 126/500 [01:29<04:23,  1.42iteration/s]\r",
      "Training:  25%|██▌       | 127/500 [01:29<04:21,  1.43iteration/s]\r",
      "Training:  26%|██▌       | 128/500 [01:30<04:20,  1.43iteration/s]\r",
      "Training:  26%|██▌       | 129/500 [01:31<04:19,  1.43iteration/s]\r",
      "Training:  26%|██▌       | 130/500 [01:32<04:18,  1.43iteration/s]\r",
      "Training:  26%|██▌       | 131/500 [01:32<04:17,  1.43iteration/s]\r",
      "Training:  26%|██▋       | 132/500 [01:33<04:17,  1.43iteration/s]\r",
      "Training:  27%|██▋       | 133/500 [01:34<04:16,  1.43iteration/s]\r",
      "Training:  27%|██▋       | 134/500 [01:34<04:15,  1.43iteration/s]\r",
      "Training:  27%|██▋       | 135/500 [01:35<04:14,  1.43iteration/s]\r",
      "Training:  27%|██▋       | 136/500 [01:36<04:14,  1.43iteration/s]\r",
      "Training:  27%|██▋       | 137/500 [01:36<04:15,  1.42iteration/s]\r",
      "Training:  28%|██▊       | 138/500 [01:37<04:14,  1.42iteration/s]\r",
      "Training:  28%|██▊       | 139/500 [01:38<04:14,  1.42iteration/s]\r",
      "Training:  28%|██▊       | 140/500 [01:39<04:13,  1.42iteration/s]\r",
      "Training:  28%|██▊       | 141/500 [01:39<04:14,  1.41iteration/s]\r",
      "Training:  28%|██▊       | 142/500 [01:40<04:14,  1.40iteration/s]\r",
      "Training:  29%|██▊       | 143/500 [01:41<04:15,  1.40iteration/s]\r",
      "Training:  29%|██▉       | 144/500 [01:41<04:14,  1.40iteration/s]\r",
      "Training:  29%|██▉       | 145/500 [01:42<04:14,  1.39iteration/s]\r",
      "Training:  29%|██▉       | 146/500 [01:43<04:13,  1.39iteration/s]\r",
      "Training:  29%|██▉       | 147/500 [01:44<04:12,  1.40iteration/s]\r",
      "Training:  30%|██▉       | 148/500 [01:44<04:13,  1.39iteration/s]\r",
      "Training:  30%|██▉       | 149/500 [01:45<04:12,  1.39iteration/s]\r",
      "Training:  30%|███       | 150/500 [01:46<04:10,  1.40iteration/s]\r",
      "Training:  30%|███       | 151/500 [01:46<04:07,  1.41iteration/s]\r",
      "Training:  30%|███       | 152/500 [01:47<04:07,  1.41iteration/s]\r",
      "Training:  31%|███       | 153/500 [01:48<04:05,  1.41iteration/s]\r",
      "Training:  31%|███       | 154/500 [01:49<04:04,  1.41iteration/s]\r",
      "Training:  31%|███       | 155/500 [01:49<04:03,  1.42iteration/s]\r",
      "Training:  31%|███       | 156/500 [01:50<04:02,  1.42iteration/s]\r",
      "Training:  31%|███▏      | 157/500 [01:51<04:02,  1.41iteration/s]\r",
      "Training:  32%|███▏      | 158/500 [01:51<04:01,  1.41iteration/s]\r",
      "Training:  32%|███▏      | 159/500 [01:52<04:01,  1.41iteration/s]\r",
      "Training:  32%|███▏      | 160/500 [01:53<03:59,  1.42iteration/s]\r",
      "Training:  32%|███▏      | 161/500 [01:53<03:58,  1.42iteration/s]\r",
      "Training:  32%|███▏      | 162/500 [01:54<03:58,  1.42iteration/s]\r",
      "Training:  33%|███▎      | 163/500 [01:55<03:57,  1.42iteration/s]\r",
      "Training:  33%|███▎      | 164/500 [01:56<03:56,  1.42iteration/s]\r",
      "Training:  33%|███▎      | 165/500 [01:56<03:55,  1.42iteration/s]\r",
      "Training:  33%|███▎      | 166/500 [01:57<03:53,  1.43iteration/s]\r",
      "Training:  33%|███▎      | 167/500 [01:58<03:52,  1.43iteration/s]\r",
      "Training:  34%|███▎      | 168/500 [01:58<03:51,  1.43iteration/s]\r",
      "Training:  34%|███▍      | 169/500 [01:59<03:50,  1.43iteration/s]\r",
      "Training:  34%|███▍      | 170/500 [02:00<03:49,  1.44iteration/s]\r",
      "Training:  34%|███▍      | 171/500 [02:00<03:48,  1.44iteration/s]\r",
      "Training:  34%|███▍      | 172/500 [02:01<03:48,  1.44iteration/s]\r",
      "Training:  35%|███▍      | 173/500 [02:02<03:48,  1.43iteration/s]\r",
      "Training:  35%|███▍      | 174/500 [02:03<03:47,  1.43iteration/s]\r",
      "Training:  35%|███▌      | 175/500 [02:03<03:46,  1.43iteration/s]\r",
      "Training:  35%|███▌      | 176/500 [02:04<03:45,  1.43iteration/s]\r",
      "Training:  35%|███▌      | 177/500 [02:05<03:45,  1.44iteration/s]\r",
      "Training:  36%|███▌      | 178/500 [02:05<03:43,  1.44iteration/s]\r",
      "Training:  36%|███▌      | 179/500 [02:06<03:42,  1.44iteration/s]\r",
      "Training:  36%|███▌      | 180/500 [02:07<03:43,  1.43iteration/s]\r",
      "Training:  36%|███▌      | 181/500 [02:07<03:42,  1.44iteration/s]\r",
      "Training:  36%|███▋      | 182/500 [02:08<03:41,  1.44iteration/s]\r",
      "Training:  37%|███▋      | 183/500 [02:09<03:40,  1.44iteration/s]\r",
      "Training:  37%|███▋      | 184/500 [02:10<03:40,  1.43iteration/s]\r",
      "Training:  37%|███▋      | 185/500 [02:10<03:40,  1.43iteration/s]\r",
      "Training:  37%|███▋      | 186/500 [02:11<03:40,  1.43iteration/s]\r",
      "Training:  37%|███▋      | 187/500 [02:12<03:39,  1.43iteration/s]\r",
      "Training:  38%|███▊      | 188/500 [02:12<03:38,  1.43iteration/s]\r",
      "Training:  38%|███▊      | 189/500 [02:13<03:36,  1.43iteration/s]\r",
      "Training:  38%|███▊      | 190/500 [02:14<03:38,  1.42iteration/s]\r",
      "Training:  38%|███▊      | 191/500 [02:14<03:36,  1.43iteration/s]\r",
      "Training:  38%|███▊      | 192/500 [02:15<03:37,  1.42iteration/s]\r",
      "Training:  39%|███▊      | 193/500 [02:16<03:36,  1.42iteration/s]\r",
      "Training:  39%|███▉      | 194/500 [02:17<03:35,  1.42iteration/s]\r",
      "Training:  39%|███▉      | 195/500 [02:17<03:34,  1.42iteration/s]\r",
      "Training:  39%|███▉      | 196/500 [02:18<03:33,  1.42iteration/s]\r",
      "Training:  39%|███▉      | 197/500 [02:19<03:32,  1.43iteration/s]\r",
      "Training:  40%|███▉      | 198/500 [02:19<03:31,  1.43iteration/s]\r",
      "Training:  40%|███▉      | 199/500 [02:20<03:30,  1.43iteration/s]\r",
      "Training:  40%|████      | 200/500 [02:21<03:29,  1.43iteration/s]\r",
      "Training:  40%|████      | 201/500 [02:21<03:28,  1.43iteration/s]\r",
      "Training:  40%|████      | 202/500 [02:22<03:27,  1.43iteration/s]\r",
      "Training:  41%|████      | 203/500 [02:23<03:26,  1.43iteration/s]\r",
      "Training:  41%|████      | 204/500 [02:24<03:26,  1.44iteration/s]\r",
      "Training:  41%|████      | 205/500 [02:24<03:24,  1.44iteration/s]\r",
      "Training:  41%|████      | 206/500 [02:25<03:24,  1.44iteration/s]\r",
      "Training:  41%|████▏     | 207/500 [02:26<03:23,  1.44iteration/s]\r",
      "Training:  42%|████▏     | 208/500 [02:26<03:23,  1.44iteration/s]\r",
      "Training:  42%|████▏     | 209/500 [02:27<03:22,  1.44iteration/s]\r",
      "Training:  42%|████▏     | 210/500 [02:28<03:22,  1.44iteration/s]\r",
      "Training:  42%|████▏     | 211/500 [02:28<03:21,  1.43iteration/s]\r",
      "Training:  42%|████▏     | 212/500 [02:29<03:22,  1.42iteration/s]\r",
      "Training:  43%|████▎     | 213/500 [02:30<03:21,  1.42iteration/s]\r",
      "Training:  43%|████▎     | 214/500 [02:31<03:20,  1.42iteration/s]\r",
      "Training:  43%|████▎     | 215/500 [02:31<03:19,  1.43iteration/s]\r",
      "Training:  43%|████▎     | 216/500 [02:32<03:23,  1.40iteration/s]\r",
      "Training:  43%|████▎     | 217/500 [02:33<03:21,  1.40iteration/s]\r",
      "Training:  44%|████▎     | 218/500 [02:33<03:20,  1.41iteration/s]\r",
      "Training:  44%|████▍     | 219/500 [02:34<03:18,  1.41iteration/s]\r",
      "Training:  44%|████▍     | 220/500 [02:35<03:17,  1.42iteration/s]\r",
      "Training:  44%|████▍     | 221/500 [02:35<03:16,  1.42iteration/s]\r",
      "Training:  44%|████▍     | 222/500 [02:36<03:15,  1.42iteration/s]\r",
      "Training:  45%|████▍     | 223/500 [02:37<03:15,  1.42iteration/s]\r",
      "Training:  45%|████▍     | 224/500 [02:38<03:13,  1.42iteration/s]\r",
      "Training:  45%|████▌     | 225/500 [02:38<03:13,  1.42iteration/s]\r",
      "Training:  45%|████▌     | 226/500 [02:39<03:27,  1.32iteration/s]\r",
      "Training:  45%|████▌     | 227/500 [02:40<03:21,  1.36iteration/s]\r",
      "Training:  46%|████▌     | 228/500 [02:41<03:17,  1.38iteration/s]\r",
      "Training:  46%|████▌     | 229/500 [02:41<03:14,  1.39iteration/s]\r",
      "Training:  46%|████▌     | 230/500 [02:42<03:12,  1.40iteration/s]\r",
      "Training:  46%|████▌     | 231/500 [02:43<03:11,  1.41iteration/s]\r",
      "Training:  46%|████▋     | 232/500 [02:43<03:09,  1.41iteration/s]\r",
      "Training:  47%|████▋     | 233/500 [02:44<03:07,  1.42iteration/s]\r",
      "Training:  47%|████▋     | 234/500 [02:45<03:06,  1.42iteration/s]\r",
      "Training:  47%|████▋     | 235/500 [02:46<03:09,  1.40iteration/s]\r",
      "Training:  47%|████▋     | 236/500 [02:46<03:07,  1.41iteration/s]\r",
      "Training:  47%|████▋     | 237/500 [02:47<03:06,  1.41iteration/s]\r",
      "Training:  48%|████▊     | 238/500 [02:48<03:05,  1.41iteration/s]\r",
      "Training:  48%|████▊     | 239/500 [02:48<03:04,  1.42iteration/s]\r",
      "Training:  48%|████▊     | 240/500 [02:49<03:03,  1.42iteration/s]\r",
      "Training:  48%|████▊     | 241/500 [02:50<03:02,  1.42iteration/s]\r",
      "Training:  48%|████▊     | 242/500 [02:50<03:02,  1.41iteration/s]\r",
      "Training:  49%|████▊     | 243/500 [02:51<03:01,  1.42iteration/s]\r",
      "Training:  49%|████▉     | 244/500 [02:52<03:00,  1.42iteration/s]\r",
      "Training:  49%|████▉     | 245/500 [02:53<02:59,  1.42iteration/s]\r",
      "Training:  49%|████▉     | 246/500 [02:53<02:58,  1.42iteration/s]\r",
      "Training:  49%|████▉     | 247/500 [02:54<02:57,  1.42iteration/s]\r",
      "Training:  50%|████▉     | 248/500 [02:55<02:56,  1.43iteration/s]\r",
      "Training:  50%|████▉     | 249/500 [02:55<02:55,  1.43iteration/s]\r",
      "Training:  50%|█████     | 250/500 [02:56<02:55,  1.43iteration/s]\r",
      "Training:  50%|█████     | 251/500 [02:57<02:56,  1.41iteration/s]\r",
      "Training:  50%|█████     | 252/500 [02:57<02:54,  1.42iteration/s]\r",
      "Training:  51%|█████     | 253/500 [02:58<02:53,  1.42iteration/s]\r",
      "Training:  51%|█████     | 254/500 [02:59<02:54,  1.41iteration/s]\r",
      "Training:  51%|█████     | 255/500 [03:00<02:52,  1.42iteration/s]\r",
      "Training:  51%|█████     | 256/500 [03:00<02:51,  1.43iteration/s]\r",
      "Training:  51%|█████▏    | 257/500 [03:01<02:50,  1.42iteration/s]\r",
      "Training:  52%|█████▏    | 258/500 [03:02<02:50,  1.42iteration/s]\r",
      "Training:  52%|█████▏    | 259/500 [03:02<02:49,  1.43iteration/s]\r",
      "Training:  52%|█████▏    | 260/500 [03:03<02:48,  1.43iteration/s]\r",
      "Training:  52%|█████▏    | 261/500 [03:04<02:47,  1.43iteration/s]\r",
      "Training:  52%|█████▏    | 262/500 [03:04<02:46,  1.43iteration/s]\r",
      "Training:  53%|█████▎    | 263/500 [03:05<02:45,  1.44iteration/s]\r",
      "Training:  53%|█████▎    | 264/500 [03:06<02:44,  1.43iteration/s]\r",
      "Training:  53%|█████▎    | 265/500 [03:07<02:43,  1.43iteration/s]\r",
      "Training:  53%|█████▎    | 266/500 [03:07<02:43,  1.43iteration/s]\r",
      "Training:  53%|█████▎    | 267/500 [03:08<02:43,  1.43iteration/s]\r",
      "Training:  54%|█████▎    | 268/500 [03:09<02:42,  1.43iteration/s]\r",
      "Training:  54%|█████▍    | 269/500 [03:09<02:42,  1.42iteration/s]\r",
      "Training:  54%|█████▍    | 270/500 [03:10<02:41,  1.42iteration/s]\r",
      "Training:  54%|█████▍    | 271/500 [03:11<02:41,  1.42iteration/s]\r",
      "Training:  54%|█████▍    | 272/500 [03:12<02:41,  1.41iteration/s]\r",
      "Training:  55%|█████▍    | 273/500 [03:12<02:40,  1.41iteration/s]\r",
      "Training:  55%|█████▍    | 274/500 [03:13<02:39,  1.42iteration/s]\r",
      "Training:  55%|█████▌    | 275/500 [03:14<02:38,  1.42iteration/s]\r",
      "Training:  55%|█████▌    | 276/500 [03:14<02:37,  1.43iteration/s]\r",
      "Training:  55%|█████▌    | 277/500 [03:15<02:38,  1.41iteration/s]\r",
      "Training:  56%|█████▌    | 278/500 [03:16<02:37,  1.41iteration/s]\r",
      "Training:  56%|█████▌    | 279/500 [03:16<02:36,  1.41iteration/s]\r",
      "Training:  56%|█████▌    | 280/500 [03:17<02:35,  1.41iteration/s]\r",
      "Training:  56%|█████▌    | 281/500 [03:18<02:34,  1.41iteration/s]\r",
      "Training:  56%|█████▋    | 282/500 [03:19<02:33,  1.42iteration/s]\r",
      "Training:  57%|█████▋    | 283/500 [03:19<02:32,  1.42iteration/s]\r",
      "Training:  57%|█████▋    | 284/500 [03:20<02:31,  1.42iteration/s]\r",
      "Training:  57%|█████▋    | 285/500 [03:21<02:31,  1.42iteration/s]\r",
      "Training:  57%|█████▋    | 286/500 [03:21<02:30,  1.42iteration/s]\r",
      "Training:  57%|█████▋    | 287/500 [03:22<02:30,  1.41iteration/s]\r",
      "Training:  58%|█████▊    | 288/500 [03:23<02:29,  1.42iteration/s]\r",
      "Training:  58%|█████▊    | 289/500 [03:24<02:28,  1.42iteration/s]\r",
      "Training:  58%|█████▊    | 290/500 [03:24<02:27,  1.43iteration/s]\r",
      "Training:  58%|█████▊    | 291/500 [03:25<02:26,  1.42iteration/s]\r",
      "Training:  58%|█████▊    | 292/500 [03:26<02:25,  1.43iteration/s]\r",
      "Training:  59%|█████▊    | 293/500 [03:26<02:25,  1.43iteration/s]\r",
      "Training:  59%|█████▉    | 294/500 [03:27<02:24,  1.42iteration/s]\r",
      "Training:  59%|█████▉    | 295/500 [03:28<02:24,  1.42iteration/s]\r",
      "Training:  59%|█████▉    | 296/500 [03:28<02:23,  1.42iteration/s]\r",
      "Training:  59%|█████▉    | 297/500 [03:29<02:24,  1.41iteration/s]\r",
      "Training:  60%|█████▉    | 298/500 [03:30<02:23,  1.41iteration/s]\r",
      "Training:  60%|█████▉    | 299/500 [03:31<02:22,  1.41iteration/s]\r",
      "Training:  60%|██████    | 300/500 [03:31<02:21,  1.41iteration/s]\r",
      "Training:  60%|██████    | 301/500 [03:32<02:20,  1.41iteration/s]\r",
      "Training:  60%|██████    | 302/500 [03:33<02:19,  1.42iteration/s]\r",
      "Training:  61%|██████    | 303/500 [03:33<02:19,  1.41iteration/s]\r",
      "Training:  61%|██████    | 304/500 [03:34<02:17,  1.42iteration/s]\r",
      "Training:  61%|██████    | 305/500 [03:35<02:16,  1.42iteration/s]\r",
      "Training:  61%|██████    | 306/500 [03:35<02:15,  1.43iteration/s]\r",
      "Training:  61%|██████▏   | 307/500 [03:36<02:15,  1.43iteration/s]\r",
      "Training:  62%|██████▏   | 308/500 [03:37<02:14,  1.43iteration/s]\r",
      "Training:  62%|██████▏   | 309/500 [03:38<02:14,  1.43iteration/s]\r",
      "Training:  62%|██████▏   | 310/500 [03:38<02:13,  1.42iteration/s]\r",
      "Training:  62%|██████▏   | 311/500 [03:39<02:12,  1.42iteration/s]\r",
      "Training:  62%|██████▏   | 312/500 [03:40<02:11,  1.43iteration/s]\r",
      "Training:  63%|██████▎   | 313/500 [03:40<02:10,  1.43iteration/s]\r",
      "Training:  63%|██████▎   | 314/500 [03:41<02:10,  1.43iteration/s]\r",
      "Training:  63%|██████▎   | 315/500 [03:42<02:11,  1.41iteration/s]\r",
      "Training:  63%|██████▎   | 316/500 [03:43<02:10,  1.41iteration/s]\r",
      "Training:  63%|██████▎   | 317/500 [03:43<02:09,  1.41iteration/s]\r",
      "Training:  64%|██████▎   | 318/500 [03:44<02:10,  1.40iteration/s]\r",
      "Training:  64%|██████▍   | 319/500 [03:45<02:09,  1.39iteration/s]\r",
      "Training:  64%|██████▍   | 320/500 [03:45<02:10,  1.38iteration/s]\r",
      "Training:  64%|██████▍   | 321/500 [03:46<02:09,  1.38iteration/s]\r",
      "Training:  64%|██████▍   | 322/500 [03:47<02:09,  1.37iteration/s]\r",
      "Training:  65%|██████▍   | 323/500 [03:48<02:08,  1.38iteration/s]\r",
      "Training:  65%|██████▍   | 324/500 [03:48<02:07,  1.38iteration/s]\r",
      "Training:  65%|██████▌   | 325/500 [03:49<02:06,  1.38iteration/s]\r",
      "Training:  65%|██████▌   | 326/500 [03:50<02:05,  1.39iteration/s]\r",
      "Training:  65%|██████▌   | 327/500 [03:50<02:03,  1.40iteration/s]\r",
      "Training:  66%|██████▌   | 328/500 [03:51<02:02,  1.40iteration/s]\r",
      "Training:  66%|██████▌   | 329/500 [03:52<02:01,  1.41iteration/s]\r",
      "Training:  66%|██████▌   | 330/500 [03:53<02:00,  1.41iteration/s]\r",
      "Training:  66%|██████▌   | 331/500 [03:53<01:59,  1.41iteration/s]\r",
      "Training:  66%|██████▋   | 332/500 [03:54<01:58,  1.41iteration/s]\r",
      "Training:  67%|██████▋   | 333/500 [03:55<01:57,  1.42iteration/s]\r",
      "Training:  67%|██████▋   | 334/500 [03:55<01:56,  1.42iteration/s]\r",
      "Training:  67%|██████▋   | 335/500 [03:56<01:55,  1.43iteration/s]\r",
      "Training:  67%|██████▋   | 336/500 [03:57<01:55,  1.42iteration/s]\r",
      "Training:  67%|██████▋   | 337/500 [03:57<01:54,  1.42iteration/s]\r",
      "Training:  68%|██████▊   | 338/500 [03:58<01:53,  1.42iteration/s]\r",
      "Training:  68%|██████▊   | 339/500 [03:59<01:52,  1.43iteration/s]\r",
      "Training:  68%|██████▊   | 340/500 [04:00<01:51,  1.43iteration/s]\r",
      "Training:  68%|██████▊   | 341/500 [04:00<01:50,  1.43iteration/s]\r",
      "Training:  68%|██████▊   | 342/500 [04:01<01:50,  1.43iteration/s]\r",
      "Training:  69%|██████▊   | 343/500 [04:02<01:50,  1.43iteration/s]\r",
      "Training:  69%|██████▉   | 344/500 [04:02<01:49,  1.42iteration/s]\r",
      "Training:  69%|██████▉   | 345/500 [04:03<01:48,  1.43iteration/s]\r",
      "Training:  69%|██████▉   | 346/500 [04:04<01:47,  1.43iteration/s]\r",
      "Training:  69%|██████▉   | 347/500 [04:04<01:46,  1.43iteration/s]\r",
      "Training:  70%|██████▉   | 348/500 [04:05<01:45,  1.43iteration/s]\r",
      "Training:  70%|██████▉   | 349/500 [04:06<01:45,  1.44iteration/s]\r",
      "Training:  70%|███████   | 350/500 [04:07<01:44,  1.44iteration/s]\r",
      "Training:  70%|███████   | 351/500 [04:07<01:43,  1.44iteration/s]\r",
      "Training:  70%|███████   | 352/500 [04:08<01:42,  1.44iteration/s]\r",
      "Training:  71%|███████   | 353/500 [04:09<01:41,  1.44iteration/s]\r",
      "Training:  71%|███████   | 354/500 [04:09<01:41,  1.44iteration/s]\r",
      "Training:  71%|███████   | 355/500 [04:10<01:41,  1.43iteration/s]\r",
      "Training:  71%|███████   | 356/500 [04:11<01:40,  1.44iteration/s]\r",
      "Training:  71%|███████▏  | 357/500 [04:11<01:39,  1.44iteration/s]\r",
      "Training:  72%|███████▏  | 358/500 [04:12<01:39,  1.43iteration/s]\r",
      "Training:  72%|███████▏  | 359/500 [04:13<01:38,  1.43iteration/s]\r",
      "Training:  72%|███████▏  | 360/500 [04:14<01:37,  1.43iteration/s]\r",
      "Training:  72%|███████▏  | 361/500 [04:14<01:37,  1.42iteration/s]\r",
      "Training:  72%|███████▏  | 362/500 [04:15<01:36,  1.42iteration/s]\r",
      "Training:  73%|███████▎  | 363/500 [04:16<01:36,  1.42iteration/s]\r",
      "Training:  73%|███████▎  | 364/500 [04:16<01:35,  1.43iteration/s]\r",
      "Training:  73%|███████▎  | 365/500 [04:17<01:34,  1.42iteration/s]\r",
      "Training:  73%|███████▎  | 366/500 [04:18<01:34,  1.42iteration/s]\r",
      "Training:  73%|███████▎  | 367/500 [04:18<01:33,  1.42iteration/s]\r",
      "Training:  74%|███████▎  | 368/500 [04:19<01:32,  1.43iteration/s]\r",
      "Training:  74%|███████▍  | 369/500 [04:20<01:31,  1.43iteration/s]\r",
      "Training:  74%|███████▍  | 370/500 [04:21<01:31,  1.43iteration/s]\r",
      "Training:  74%|███████▍  | 371/500 [04:21<01:30,  1.43iteration/s]\r",
      "Training:  74%|███████▍  | 372/500 [04:22<01:30,  1.42iteration/s]\r",
      "Training:  75%|███████▍  | 373/500 [04:23<01:29,  1.43iteration/s]\r",
      "Training:  75%|███████▍  | 374/500 [04:23<01:28,  1.43iteration/s]\r",
      "Training:  75%|███████▌  | 375/500 [04:24<01:27,  1.43iteration/s]\r",
      "Training:  75%|███████▌  | 376/500 [04:25<01:26,  1.43iteration/s]\r",
      "Training:  75%|███████▌  | 377/500 [04:25<01:25,  1.44iteration/s]\r",
      "Training:  76%|███████▌  | 378/500 [04:26<01:25,  1.43iteration/s]\r",
      "Training:  76%|███████▌  | 379/500 [04:27<01:25,  1.41iteration/s]\r",
      "Training:  76%|███████▌  | 380/500 [04:28<01:24,  1.42iteration/s]\r",
      "Training:  76%|███████▌  | 381/500 [04:28<01:23,  1.42iteration/s]\r",
      "Training:  76%|███████▋  | 382/500 [04:29<01:22,  1.42iteration/s]\r",
      "Training:  77%|███████▋  | 383/500 [04:30<01:22,  1.43iteration/s]\r",
      "Training:  77%|███████▋  | 384/500 [04:30<01:21,  1.43iteration/s]\r",
      "Training:  77%|███████▋  | 385/500 [04:31<01:20,  1.43iteration/s]\r",
      "Training:  77%|███████▋  | 386/500 [04:32<01:20,  1.42iteration/s]\r",
      "Training:  77%|███████▋  | 387/500 [04:33<01:19,  1.42iteration/s]\r",
      "Training:  78%|███████▊  | 388/500 [04:33<01:18,  1.42iteration/s]\r",
      "Training:  78%|███████▊  | 389/500 [04:34<01:18,  1.42iteration/s]\r",
      "Training:  78%|███████▊  | 390/500 [04:35<01:17,  1.42iteration/s]\r",
      "Training:  78%|███████▊  | 391/500 [04:35<01:16,  1.42iteration/s]\r",
      "Training:  78%|███████▊  | 392/500 [04:36<01:15,  1.43iteration/s]\r",
      "Training:  79%|███████▊  | 393/500 [04:37<01:15,  1.42iteration/s]\r",
      "Training:  79%|███████▉  | 394/500 [04:37<01:14,  1.43iteration/s]\r",
      "Training:  79%|███████▉  | 395/500 [04:38<01:13,  1.43iteration/s]\r",
      "Training:  79%|███████▉  | 396/500 [04:39<01:12,  1.44iteration/s]\r",
      "Training:  79%|███████▉  | 397/500 [04:40<01:11,  1.44iteration/s]\r",
      "Training:  80%|███████▉  | 398/500 [04:40<01:10,  1.44iteration/s]\r",
      "Training:  80%|███████▉  | 399/500 [04:41<01:10,  1.43iteration/s]\r",
      "Training:  80%|████████  | 400/500 [04:42<01:09,  1.43iteration/s]\r",
      "Training:  80%|████████  | 401/500 [04:42<01:09,  1.43iteration/s]\r",
      "Training:  80%|████████  | 402/500 [04:43<01:08,  1.42iteration/s]\r",
      "Training:  81%|████████  | 403/500 [04:44<01:09,  1.39iteration/s]\r",
      "Training:  81%|████████  | 404/500 [04:44<01:08,  1.41iteration/s]\r",
      "Training:  81%|████████  | 405/500 [04:45<01:07,  1.40iteration/s]\r",
      "Training:  81%|████████  | 406/500 [04:46<01:06,  1.41iteration/s]\r",
      "Training:  81%|████████▏ | 407/500 [04:47<01:05,  1.41iteration/s]\r",
      "Training:  82%|████████▏ | 408/500 [04:47<01:05,  1.41iteration/s]\r",
      "Training:  82%|████████▏ | 409/500 [04:48<01:04,  1.42iteration/s]\r",
      "Training:  82%|████████▏ | 410/500 [04:49<01:03,  1.42iteration/s]\r",
      "Training:  82%|████████▏ | 411/500 [04:49<01:02,  1.42iteration/s]\r",
      "Training:  82%|████████▏ | 412/500 [04:50<01:01,  1.42iteration/s]\r",
      "Training:  83%|████████▎ | 413/500 [04:51<01:01,  1.42iteration/s]\r",
      "Training:  83%|████████▎ | 414/500 [04:52<01:00,  1.42iteration/s]\r",
      "Training:  83%|████████▎ | 415/500 [04:52<00:59,  1.42iteration/s]\r",
      "Training:  83%|████████▎ | 416/500 [04:53<00:58,  1.42iteration/s]\r",
      "Training:  83%|████████▎ | 417/500 [04:54<00:58,  1.43iteration/s]\r",
      "Training:  84%|████████▎ | 418/500 [04:54<00:57,  1.43iteration/s]\r",
      "Training:  84%|████████▍ | 419/500 [04:55<00:56,  1.43iteration/s]\r",
      "Training:  84%|████████▍ | 420/500 [04:56<00:55,  1.43iteration/s]\r",
      "Training:  84%|████████▍ | 421/500 [04:56<00:55,  1.43iteration/s]\r",
      "Training:  84%|████████▍ | 422/500 [04:57<00:54,  1.43iteration/s]\r",
      "Training:  85%|████████▍ | 423/500 [04:58<00:54,  1.42iteration/s]\r",
      "Training:  85%|████████▍ | 424/500 [04:59<00:53,  1.42iteration/s]\r",
      "Training:  85%|████████▌ | 425/500 [04:59<00:53,  1.41iteration/s]\r",
      "Training:  85%|████████▌ | 426/500 [05:00<00:52,  1.42iteration/s]\r",
      "Training:  85%|████████▌ | 427/500 [05:01<00:51,  1.42iteration/s]\r",
      "Training:  86%|████████▌ | 428/500 [05:01<00:50,  1.42iteration/s]\r",
      "Training:  86%|████████▌ | 429/500 [05:02<00:50,  1.42iteration/s]\r",
      "Training:  86%|████████▌ | 430/500 [05:03<00:49,  1.42iteration/s]\r",
      "Training:  86%|████████▌ | 431/500 [05:03<00:48,  1.42iteration/s]\r",
      "Training:  86%|████████▋ | 432/500 [05:04<00:47,  1.42iteration/s]\r",
      "Training:  87%|████████▋ | 433/500 [05:05<00:47,  1.42iteration/s]\r",
      "Training:  87%|████████▋ | 434/500 [05:06<00:46,  1.43iteration/s]\r",
      "Training:  87%|████████▋ | 435/500 [05:06<00:45,  1.42iteration/s]\r",
      "Training:  87%|████████▋ | 436/500 [05:07<00:45,  1.42iteration/s]\r",
      "Training:  87%|████████▋ | 437/500 [05:08<00:44,  1.42iteration/s]\r",
      "Training:  88%|████████▊ | 438/500 [05:08<00:43,  1.43iteration/s]\r",
      "Training:  88%|████████▊ | 439/500 [05:09<00:42,  1.43iteration/s]\r",
      "Training:  88%|████████▊ | 440/500 [05:10<00:42,  1.43iteration/s]\r",
      "Training:  88%|████████▊ | 441/500 [05:10<00:41,  1.43iteration/s]\r",
      "Training:  88%|████████▊ | 442/500 [05:11<00:40,  1.43iteration/s]\r",
      "Training:  89%|████████▊ | 443/500 [05:12<00:40,  1.42iteration/s]\r",
      "Training:  89%|████████▉ | 444/500 [05:13<00:39,  1.42iteration/s]\r",
      "Training:  89%|████████▉ | 445/500 [05:13<00:38,  1.43iteration/s]\r",
      "Training:  89%|████████▉ | 446/500 [05:14<00:37,  1.43iteration/s]\r",
      "Training:  89%|████████▉ | 447/500 [05:15<00:37,  1.43iteration/s]\r",
      "Training:  90%|████████▉ | 448/500 [05:15<00:36,  1.43iteration/s]\r",
      "Training:  90%|████████▉ | 449/500 [05:16<00:35,  1.43iteration/s]\r",
      "Training:  90%|█████████ | 450/500 [05:17<00:35,  1.41iteration/s]\r",
      "Training:  90%|█████████ | 451/500 [05:18<00:34,  1.42iteration/s]\r",
      "Training:  90%|█████████ | 452/500 [05:18<00:33,  1.42iteration/s]\r",
      "Training:  91%|█████████ | 453/500 [05:19<00:32,  1.43iteration/s]\r",
      "Training:  91%|█████████ | 454/500 [05:20<00:32,  1.43iteration/s]\r",
      "Training:  91%|█████████ | 455/500 [05:20<00:31,  1.43iteration/s]\r",
      "Training:  91%|█████████ | 456/500 [05:21<00:30,  1.43iteration/s]\r",
      "Training:  91%|█████████▏| 457/500 [05:22<00:30,  1.43iteration/s]\r",
      "Training:  92%|█████████▏| 458/500 [05:22<00:29,  1.43iteration/s]\r",
      "Training:  92%|█████████▏| 459/500 [05:23<00:28,  1.43iteration/s]\r",
      "Training:  92%|█████████▏| 460/500 [05:24<00:27,  1.43iteration/s]\r",
      "Training:  92%|█████████▏| 461/500 [05:24<00:27,  1.44iteration/s]\r",
      "Training:  92%|█████████▏| 462/500 [05:25<00:26,  1.43iteration/s]\r",
      "Training:  93%|█████████▎| 463/500 [05:26<00:25,  1.43iteration/s]\r",
      "Training:  93%|█████████▎| 464/500 [05:27<00:25,  1.43iteration/s]\r",
      "Training:  93%|█████████▎| 465/500 [05:27<00:24,  1.42iteration/s]\r",
      "Training:  93%|█████████▎| 466/500 [05:28<00:23,  1.42iteration/s]\r",
      "Training:  93%|█████████▎| 467/500 [05:29<00:23,  1.40iteration/s]\r",
      "Training:  94%|█████████▎| 468/500 [05:29<00:22,  1.41iteration/s]\r",
      "Training:  94%|█████████▍| 469/500 [05:30<00:21,  1.42iteration/s]\r",
      "Training:  94%|█████████▍| 470/500 [05:31<00:21,  1.42iteration/s]\r",
      "Training:  94%|█████████▍| 471/500 [05:32<00:20,  1.42iteration/s]\r",
      "Training:  94%|█████████▍| 472/500 [05:32<00:19,  1.42iteration/s]\r",
      "Training:  95%|█████████▍| 473/500 [05:33<00:19,  1.42iteration/s]\r",
      "Training:  95%|█████████▍| 474/500 [05:34<00:18,  1.42iteration/s]\r",
      "Training:  95%|█████████▌| 475/500 [05:34<00:17,  1.43iteration/s]\r",
      "Training:  95%|█████████▌| 476/500 [05:35<00:16,  1.43iteration/s]\r",
      "Training:  95%|█████████▌| 477/500 [05:36<00:16,  1.43iteration/s]\r",
      "Training:  96%|█████████▌| 478/500 [05:37<00:15,  1.39iteration/s]\r",
      "Training:  96%|█████████▌| 479/500 [05:37<00:15,  1.39iteration/s]\r",
      "Training:  96%|█████████▌| 480/500 [05:38<00:14,  1.41iteration/s]\r",
      "Training:  96%|█████████▌| 481/500 [05:39<00:13,  1.41iteration/s]\r",
      "Training:  96%|█████████▋| 482/500 [05:39<00:12,  1.42iteration/s]\r",
      "Training:  97%|█████████▋| 483/500 [05:40<00:11,  1.43iteration/s]\r",
      "Training:  97%|█████████▋| 484/500 [05:41<00:11,  1.43iteration/s]\r",
      "Training:  97%|█████████▋| 485/500 [05:41<00:10,  1.43iteration/s]\r",
      "Training:  97%|█████████▋| 486/500 [05:42<00:09,  1.43iteration/s]\r",
      "Training:  97%|█████████▋| 487/500 [05:43<00:09,  1.43iteration/s]\r",
      "Training:  98%|█████████▊| 488/500 [05:44<00:08,  1.43iteration/s]\r",
      "Training:  98%|█████████▊| 489/500 [05:44<00:07,  1.43iteration/s]\r",
      "Training:  98%|█████████▊| 490/500 [05:45<00:06,  1.43iteration/s]\r",
      "Training:  98%|█████████▊| 491/500 [05:46<00:06,  1.42iteration/s]\r",
      "Training:  98%|█████████▊| 492/500 [05:46<00:05,  1.43iteration/s]\r",
      "Training:  99%|█████████▊| 493/500 [05:47<00:04,  1.42iteration/s]\r",
      "Training:  99%|█████████▉| 494/500 [05:48<00:04,  1.43iteration/s]\r",
      "Training:  99%|█████████▉| 495/500 [05:48<00:03,  1.42iteration/s]\r",
      "Training:  99%|█████████▉| 496/500 [05:49<00:02,  1.42iteration/s]\r",
      "Training:  99%|█████████▉| 497/500 [05:50<00:02,  1.41iteration/s]\r",
      "Training: 100%|█████████▉| 498/500 [05:51<00:01,  1.40iteration/s]\r",
      "Training: 100%|█████████▉| 499/500 [05:51<00:00,  1.40iteration/s]\r",
      "Training: 100%|██████████| 500/500 [05:52<00:00,  1.40iteration/s]\r",
      "Training: 100%|██████████| 500/500 [05:52<00:00,  1.42iteration/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize learning rate and maximum iterations\n",
    "learning_rate = 0.31101\n",
    "max_iterations = 500\n",
    "\n",
    "# Train the model\n",
    "weights, bias = train_multinomial_logistic_regression(X_train, y_train, learning_rate, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PO4I6kCz0Wpw"
   },
   "source": [
    "## Task 16: Test the Model\n",
    "\n",
    "In this task, you’ll use the predict() function with the updated weights and bias from the last task to predict the class labels for the test data. \n",
    "The predict() function will compute the scores and probabilities for each sample in the test data and return the predicted class labels.\n",
    "\n",
    "To complete this task, follow the given steps:\n",
    "\n",
    "1. Invoke the predict() function using the test data and store the predicted class labels as y_pred.\n",
    "    \n",
    "2. Print y_pred, the predicted class labels for the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-n4fK7I80bRG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 2 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "y_pred = predict(X_test, weights, bias)\n",
    "\n",
    "# Print the predicted class labels\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    511\n",
       "0    345\n",
       "2    344\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AjfowMRmZwx"
   },
   "source": [
    "## Task 17: Generate the Confusion Matrix and Classification Report\n",
    "\n",
    "In this task, generate a confusion matrix based on the true and predicted labels. Furthermore, you’ll print a classification report that summarizes \n",
    "the performance of the multinomial logistic regression model on the test data. This report will include metrics for each class label,\n",
    "providing insights into precision, recall, F1-score, and support.\n",
    "\n",
    "Follow the given steps to complete this task:\n",
    "\n",
    "1. Generate a confusion matrix based on the true labels, y_test, and the predicted labels, y_pred, obtained from the multinomial logistic regression\n",
    "model.\n",
    "\n",
    "2. Plot the confusion matrix.\n",
    "\n",
    "3. Generate a classification report using the true labels, y_test, and the predicted labels, y_pred.\n",
    "\n",
    "4. Print the classification report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wYAva2Re4YKu"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.63      0.54      0.58       400\n",
      "    Negative       0.47      0.60      0.52       400\n",
      "     Neutral       0.62      0.54      0.58       400\n",
      "\n",
      "    accuracy                           0.56      1200\n",
      "   macro avg       0.57      0.56      0.56      1200\n",
      "weighted avg       0.57      0.56      0.56      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "## Generate a classification report\n",
    "print(classification_report(y_test, y_pred, target_names=['Positive', 'Negative', 'Neutral']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1i5MgXs8N1kzz7cmSf0yUGVFcZKqpopeZ",
     "timestamp": 1684430188484
    },
    {
     "file_id": "1kglHaNMqI8RJ2xs6Ki0CQbuhQw_Hq0_y",
     "timestamp": 1684429013404
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
